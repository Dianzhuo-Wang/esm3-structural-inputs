
import sys
sys.path.append("./esm3")

import numpy as np
import torch


from esm.utils.structure.protein_chain import ProteinChain
from esm.models.esm3 import ESM3
from esm.sdk import client
from esm.sdk.api import (
    ESMProtein
)
from esm.utils import encoding

import pandas as pd

import os
from tqdm import tqdm
import argparse
from math import ceil


# Argparse
parser = argparse.ArgumentParser(description='Embed sequences with ESM3')
parser.add_argument('--chain-id', type=str, required=True,
                    help='The chain id of the protein')
parser.add_argument('--embeddings-save-file-name', type=str, required=True,
                    help='The name of the file where to save the embeddings, ex: cov555.pt')
parser.add_argument('--suffix-file-to-embed', type=str, required=True,
                    help='The suffix of the file to embed, ex: _first_relax.pdb')
parser.add_argument('--directory-structure', type=str, required=True,
                    help='The directory where the structures are stored, ex: protein-structures/mutant-evoEf-6xf5-rbd-only/')
parser.add_argument('--sequences-to-embed', type=str, required=True,
                    help='Sequences to embed, ex: df_Desai_15loci_sequence.npy')
parser.add_argument('--folder-for-embeddings', type=str, required=True,
                    help='Folder to load sequences and save embeddings, ex: data')

args = parser.parse_args()

chain_id = args.chain_id
# folder_for_embeddings = "data"
folder_for_embeddings = args.folder_for_embeddings
embeddings_save_file_name = args.embeddings_save_file_name
# You should not change this
# you can change this
suffix_file_to_embed=args.suffix_file_to_embed
save_folder = "embedding_savings_" + embeddings_save_file_name.split(".")[0] + suffix_file_to_embed.split(".")[0] + folder_for_embeddings # remove the extension
# Make sure to choose the right indexes (you can check interactively with check-protein-structure.ipynb)
rbd_residue_index_starting_desai_sequences = 331
rbd_residue_index_ending_desai_sequences = 531 # included
index_start = 3
index_stop = -5
directory_structure=args.directory_structure
### Make sure to choose the chain_id


# # Create ESMProteins and embeddings for desai old


model =  ESM3.from_pretrained("esm3_sm_open_v1", device=torch.device("cuda"))

# sequences_to_embed = folder_for_embeddings + "/" + "df_Desai_15loci_sequence.npy"
sequences_to_embed = folder_for_embeddings + "/" + args.sequences_to_embed
sequences = np.load(sequences_to_embed, allow_pickle=True)

files = [f.path for f in os.scandir(directory_structure) if f.is_dir()]
print(f'Found {len(files)} files in the directory')

# Make batch of 50 examples
batch_size = 10
n_batches = ceil(len(files) / batch_size)

def get_esm_protein(index, chain_id, suffix_file_to_embed):
    """
    Load the correct file in a folder generated by Rosetta Pipeline

    folder_name: str, eg. "protein-structures/mutant-evoEf-6xf5-rbd-only/1" 
    """
    folder_name = args.directory_structure + "/" + str(index)
    name_file_to_load = f"{index}{suffix_file_to_embed}"
    protein_chain = ProteinChain.from_pdb(folder_name + "/" + name_file_to_load, chain_id=chain_id)
    protein_chain = protein_chain.select_residue_indices(
        range(rbd_residue_index_starting_desai_sequences + index_start,
        rbd_residue_index_ending_desai_sequences + index_stop + 1)
    )
    esm_protein = ESMProtein.from_protein_chain(protein_chain)
    esm_protein.sequence = sequences[index-1][index_start:index_stop]
    return esm_protein

if not os.path.exists(save_folder):
    os.makedirs(save_folder)


with torch.no_grad():
    for i in tqdm(range(n_batches)):
        #print(f"Processing batch {i}")
        if os.path.exists(f"{save_folder}/embeddings_{i}_withCoordinates.pt"):
            continue
        start = i*batch_size + 1
        end = min((i+1)*batch_size, len(files)-1) + 1

        batch = range(start, end)
        batch = [get_esm_protein(index, chain_id, suffix_file_to_embed) for index in batch]
        batch = [model.encode(seq) for seq in batch]

        # batch = model.encode_batch(batch)
        embeddings, embeddings_norm_layer = model.get_embeddings_batched(batch)

        max_pooling_embeddings = embeddings.max(dim=1).values
        embeddings = embeddings.mean(dim=1)

        torch.save(embeddings, f"{save_folder}/embeddings_{i}_withCoordinates.pt")
        torch.save(max_pooling_embeddings, f"{save_folder}/embeddings_{i}_max_pooling.pt")


embeddings_withCoordinates = []
embeddings_max_pooling = []

for i in range(n_batches):
    embeddings_max_pooling.append(torch.load(f"{save_folder}/embeddings_{i}_max_pooling.pt"))
    embeddings_withCoordinates.append(torch.load(f"{save_folder}/embeddings_{i}_withCoordinates.pt"))

embeddings_withCoordinates = torch.cat(embeddings_withCoordinates, dim=0)
embeddings_max_pooling = torch.cat(embeddings_max_pooling, dim=0)


torch.save(embeddings_withCoordinates, folder_for_embeddings + "/embeddings_withCoordinates_" + embeddings_save_file_name +".pt")
torch.save(embeddings_max_pooling, folder_for_embeddings + "/embeddings_max_pooling_" + embeddings_save_file_name +".pt")
